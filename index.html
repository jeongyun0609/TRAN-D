<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="Sparse view, transparent, depth reconstruction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>2D Gaussian Splatting-based Sparse-view Transparent Object
    Depth Reconstruction via Physics Simulation for Scene Update</title>


  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/4.0.0/model-viewer.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-YXDZHQRVSJ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-YXDZHQRVSJ');
</script>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title is-2 publication-title">2D Gaussian Splatting-based Sparse-view Transparent Object<br>
            Depth Reconstruction via Physics Simulation for Scene Update</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://jeongyun0609.github.io/" target="_blank">Jeongyun Kim<sup>1</sup></a>,</span>
            <span class="author-block">
              <a >Seunghoon Jeong<sup>1</sup></a>,</span>
            <span class="author-block">
              <a href="https://gisbi-kim.github.io/" target="_blank">Giseop Kim<sup>2</sup></a>,</span>
            <span class="author-block">
              <a href="https://myunghwanjeon.github.io/" target="_blank">Myung-Hwan Jeon<sup>3</sup></a>,</span>
            <span class="author-block">
              <a >Eunji Jun<sup>4</sup></a>,</span>
            <span class="author-block">
              <a href="https://ayoungk.github.io/" target="_blank">Ayoung Kim<sup>1</sup></a></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Seoul National University, <sup>2</sup>DGIST, <sup>3</sup>University of Illinois Urbana-Champaign, <sup>4</sup> Hyundai Motor Group</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">ICCV 2025</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a >
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/jeongyun0609/TRAN-D"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Understanding the 3D geometry of transparent objects from RGB images is challenging due to their inherent physical properties, such as reflection and refraction. To address these difficulties, especially in scenarios with sparse views and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian Splatting-based depth reconstruction method for transparent objects.
            Our key insight lies in separating transparent objects from the background, enabling focused optimization of Gaussians corresponding to the object. We mitigate artifacts with an object-aware loss that places Gaussians in obscured regions, ensuring coverage of invisible surfaces while reducing overfitting. Furthermore, we incorporate a physics-based simulation that refines the reconstruction in just a few seconds, effectively handling object removal and chain-reaction movement of remaining objects without the need for rescanning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method Overview</h2>
        <div class="content has-text-justified">
          <p>
            TRAN-D is a three-stage framework for reconstructing transparent objects from sparse RGB views. 
            First, the <em>segmentation module</em> isolates transparent object instances using Grounded SAM, trained with a category-specific prompting strategy. 
            Next, 2D Gaussians are randomly initialized and optimized in the <em>object-aware 2D Gaussian Splatting module</em> using differentiable tile rasterization and a novel object-aware 3D loss. 
            This step produces dense and artifact-free object reconstructions. 
            Finally, the <em>scene update module</em> employs physics-based simulation to refine the reconstruction after object removal, handling chain-reaction movements without requiring re-scanning.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./images/figure2.pdf" alt="Method figure."/>
        </div>
      </div>
    </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2>
        <!-- Subsection. -->
        <h3 class="title is-4">Unknown object depth reconstruction results of synthetic sequences</h3>
      <!-- Flexbox container for 3 items in a row -->
      <div style="display: flex; justify-content: center; gap: 1.5rem; flex-wrap: wrap;">
        <!-- Item 1 -->
        <div style="text-align: center; width: 30%;">
          <img src="./images/textureless1_0.png" alt="Result 1" style="width: 100%;"/>
          <video style="width: 100%; margin-top: 0.5rem;" controls muted loop>
            <source src="./videos/1_0.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>

        <!-- Item 2 -->
        <div style="text-align: center; width: 30%;">
          <img src="./images/textureless2_0.png" alt="Result 2" style="width: 100%;"/>
          <video style="width: 100%; margin-top: 0.5rem;" controls muted loop>
            <source src="./videos/2_0.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>

        <!-- Item 3 -->
        <div style="text-align: center; width: 30%;">
          <img src="./images/textureless3_0.png" alt="Result 3" style="width: 100%;"/>
          <video style="width: 100%; margin-top: 0.5rem;" controls muted loop>
            <source src="./videos/3_0.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>
        <div class="content has-text-justified"> 
          <p></p>
        </div>

      

        <!-- Subsection. -->
        <h3 class="title is-4">Qualitative Comparison of Predicted Geometry and Camera Poses</h3>
        <div class="content has-text-justified">
          <p>
            DiffusionSfM demonstrates robust performance even with challenging inputs. Compared to DUSt3R, which sometimes fails to register images in a consistent manner, DiffusionSfM consistently yields a coherent global prediction. Additionally, while we observe that DUSt3R can predict highly precise camera rotations, it often struggles with camera centers (see the backpack example). Input images depicting scenes are out-of-distribution for RayDiffusion, as it is trained on CO3D only.
          </p>
        </div>
        <div class="content has-text-centered">
            <img src="./research/DiffusionSfM/figures/vis_compare.jpg" alt="LEAP Comparison figure." width="100%"/>
        </div>

        <!-- Subsection. -->
        <div class="content has-text-centered">
            <img src="./research/DiffusionSfM/figures/vis_cameras.png" alt="LEAP Comparison figure." width="100%"/>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Discussion</h2>
    <p>
      We present DiffusionSfM and demonstrate that it recovers accurate predictions of both cameras and geometry from multi-view inputs. Although our results are promising, several challenges and open questions remain. 
    </p>
    <p>
      Notably, DiffusionSfM employs a pixel-space diffusion model, in contrast to the latent-space models adopted by state-of-the-art T2I generative systems. Operating in pixel space may require greater model capacity, yet our current model remains relatively small -- potentially explaining the noisy patterns observed along object boundaries. Learning an expressive latent space for ray origins and endpoints by training a VAE could be a promising direction for future work. In addition, the computational requirement in multi-view transformers scales quadratically with the number of input images: one would require masked attention to deploy systems like ours for a large set of input images. 
    </p>
    <p>
      Despite these challenges, we believe that our work highlights the potential of a unified approach for multi-view geometry tasks. We envision that our approach can be built upon to train a common system across related geometric tasks, such as SfM (input images with unknown origins and endpoints), registration (some images have known origins and endpoints, whereas others don't), mapping (known rays but unknown endpoints), and view synthesis (unknown pixel values for known rays). </p>
    </p>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">BibTeX</h2>
<pre><code>@inproceedings{zhao2025diffusionsfm,
  title={DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion}, 
  author={Qitao Zhao and Amy Lin and Jeff Tan and Jason Y. Zhang and Deva Ramanan and Shubham Tulsiani},
  booktitle={CVPR},
  year={2025} 
}</code></pre>
  </div>
</section>


<section class="column" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Acknowledgements</h2>
    <p>
      We thank the members of the Physical Perception Lab at CMU for their valuable discussions, and extend special thanks to Yanbo Xu for his insights on diffusion models.
    </p>
    <p>
      This work used Bridges-2 at Pittsburgh Supercomputing Center through allocation CIS240166 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. This work was supported by Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) contract number 140D0423C0074. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DOI/IBC, or the U.S. Government.
    </p>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2505.05473">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/QitaoZhao/DiffusionSfM" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is built using this template (<a href="https://github.com/nerfies/nerfies.github.io">source code</a>). The 3D model visualization is adapted from <a href="https://vgg-t.github.io/">VGGT</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
